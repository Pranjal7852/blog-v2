[{"content":"Privacy, Cost, and Control: Why We Moved Away from Third-Party AI Services During my time working on a social media platform for kids, I was tasked with working on a service to moderate text messages. I briefly remember from one of our product meetings we discussed that the chat should have at least these functionalities for kids: auto-correction (just like Grammarly) and toxicity detection to make the platform safe for children to use.\nThe goal was absolutely clear: make the chat system resonate with the product\u0026rsquo;s mission—safe for children—and add value to their learning. And like every other startup in the game, we also wanted to ride the AI gold wagon.\nThe Initial AI Approach We worked on functionality where we sent user text in a throttled mode to an AI API that would tell us about grammatical errors in sentences and give us a toxicity rating for the text. At first, it seemed like a great deal: with a single request, we could get solutions to multiple problems with very minimal infrastructure setup. But soon, that excitement started to fade away.\nThe Reality Check We soon realized that sending user chats to third-party companies like OpenAI or Claude was not a good idea for obvious privacy concerns—and definitely not appropriate for a child-safe platform. Now you may ask: \u0026ldquo;This can be easily solved by self-hosting your own open-source LLM.\u0026rdquo; But wait for it.\nCurrently, it was hard for us to monitor AI responses effectively. There was no framework present to know whether the responses provided by the LLM were always accurate, what the error rate was, or—as some experts have coined—what the hallucination rate was. For which test inputs did the AI not work as it should?\nAdditionally, the cost of tokens was far more than we had imagined. This meant that for every chat happening on the platform, we could potentially lose money. So while using AI seemed to be a good choice at first, it definitely was not when we considered all factors.\nImportant Clarification I\u0026rsquo;m not saying that using AI is bad or that you should avoid it entirely. What I\u0026rsquo;m trying to convey is that jumping to AI solutions without proper thought and design can be catastrophic. At first, it may look like the best approach for a given problem, but it often isn\u0026rsquo;t when you account for the various other factors that constitute a complete solution.\nOur Solution After around a day of intensive discussion, we agreed to use open-source models for grammar correction and toxicity detection. We decided to create a TensorFlow FastAPI wrapper, dockerize it inside a container, and run it on our EC2 instances.\nBy self-hosting these machine learning models, we solved several critical issues:\nPrivacy Concerns Resolved: The data never left our infrastructure, ensuring complete privacy protection for our young users.\nCost Efficiency: There were no token costs per API transaction. We were only paying for the instance setup, which was significantly less expensive compared to API subscription costs.\nComplete Control: We had full control of the service and comprehensive knowledge about API throughput and latency. We could monitor performance, adjust parameters, and optimize for our specific use case.\nReliability: We could guarantee uptime and performance without depending on external services that might experience outages or rate limiting.\nKey Takeaways This solution proved to be fast, inexpensive, and gave us the control and transparency that we lacked when using third-party AI services. The experience taught us valuable lessons about evaluating technology decisions holistically, considering not just the immediate technical benefits but also long-term implications for privacy, cost, control, and reliability.\nSometimes the most sophisticated solution isn\u0026rsquo;t the best solution—especially for startups where every decision can significantly impact both user trust and financial sustainability.\n","permalink":"http://localhost:1313/posts/ai-goldrush/","summary":"\u003ch1 id=\"privacy-cost-and-control-why-we-moved-away-from-third-party-ai-services\"\u003ePrivacy, Cost, and Control: Why We Moved Away from Third-Party AI Services\u003c/h1\u003e\n\u003cp\u003eDuring my time working on a social media platform for kids, I was tasked with working on a service to moderate text messages. I briefly remember from one of our product meetings we discussed that the chat should have at least these functionalities for kids: auto-correction (just like Grammarly) and toxicity detection to make the platform safe for children to use.\u003c/p\u003e","title":"Beyond the AI Gold Rush"},{"content":"Custom solutions to enhance Search Enginer Optimization and an Overview of Google Seach Console Imagine this: Your web app is ready to rock the world. You’ve nailed the development, deployment is flawless, and your marketing team is prepped to take the spotlight. But there’s a catch—when users Google your app’s name, your website doesn’t show up. How can users trust your app is legit? And if they want to test your product, how will they find it? You can’t just hand out your server’s IP address to bypass DNS, right?\nLet’s dive into this problem and explore how I helped our site rank at the top of Google’s search results.\nWhat is Indexing, and Why Does It Matter? Before we dive into solutions, let’s address the basics. What is indexing?\nIndexing is the process by which search engine bots (also known as crawlers) store your site’s content—like text, images, and metadata—in their database. This allows Google to quickly retrieve and display relevant results when users search for something.\nBut what if your site isn’t being indexed properly? That’s where Google Search Console (GSC) and a few SEO best practices come into play.\nProblem 1: Canonical URLs for Multi-Region Websites If your website has multiple versions—say, one for the US (example.us), one for Japan (example.jp), and another for India (example.in)—you need to ensure users see the correct version based on their location. You don’t want a user in the US landing on the Japanese version of your site, do you?\nSolution: Dynamically Generate Sitemaps and Robots.txt Files To tackle this, we implemented a dynamic sitemap and robots.txt generation system. Here’s how it works:\nEnvironment-Based Sitemap Generation:\nWe used environment variables (e.g., REGION=jp) to generate sitemaps tailored to each region. For example, if REGION=jp, the sitemap would prioritize https://example.jp as the canonical URL. Dynamic URLs for Content:\nWe also fetched data from the server to dynamically include all relevant pages in the sitemap. This ensures every page is indexed without manual intervention. Here’s an example of how we implemented this in Next.js:\nfunction generateSiteMap(worksheets) { return `\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34;\u0026gt; \u0026lt;!-- Static URLs --\u0026gt; \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;https://example.${process.env.REGION}\u0026lt;/loc\u0026gt; \u0026lt;changefreq\u0026gt;weekly\u0026lt;/changefreq\u0026gt; \u0026lt;priority\u0026gt;1.0\u0026lt;/priority\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;https://example.ai/home\u0026lt;/loc\u0026gt; \u0026lt;changefreq\u0026gt;daily\u0026lt;/changefreq\u0026gt; \u0026lt;priority\u0026gt;0.8\u0026lt;/priority\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;https://example.ai/onboarding\u0026lt;/loc\u0026gt; \u0026lt;changefreq\u0026gt;weekly\u0026lt;/changefreq\u0026gt; \u0026lt;priority\u0026gt;0.7\u0026lt;/priority\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;!-- Dynamic URLs from worksheet data --\u0026gt; ${worksheets .map(({ worksheet_id, activity_id }) =\u0026gt; { return ` \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;${`https://example.ai/home/worksheet/${worksheet_id}/${activity_id}`}\u0026lt;/loc\u0026gt; \u0026lt;changefreq\u0026gt;monthly\u0026lt;/changefreq\u0026gt; \u0026lt;priority\u0026gt;0.6\u0026lt;/priority\u0026gt; \u0026lt;/url\u0026gt; `; }) .join(\u0026#34;\u0026#34;)} \u0026lt;/urlset\u0026gt;`; } Additional Methods to Set Canonical URLs Add a rel=canonical link to your HTML file. Send an HTML header with rel=canonical. Include rel=canonical in your sitemap for all builds. Problem 2: Preventing Test or Beta Sites from Being Indexed You don’t want your testing or beta versions popping up on Google, right? These versions aren’t meant for end users, and indexing them can lead to confusion.\nSolution: Use Robots.txt to Disallow Crawling By adding test or beta site URLs to your robots.txt file, you can instruct search engine bots to skip these pages. Here’s an example:\n# Allow all search engines to crawl the entire site User-agent: * Disallow: /api.us.example.ai/ Disallow: /graphql.us.example.ai/ Disallow: /home/worksheet/ Disallow: /proxy.mixpanel.example.ai/ # Allow Googlebot to access the entire site User-agent: Googlebot Disallow: # Allow Bingbot to access the entire site User-agent: Bingbot Disallow: # Sitemap location Sitemap: http://www.exmaple.ai/sitemap.xml How Google Search Console (GSC) Can Help Google Search Console is a powerful tool for developers and marketers alike. It helps you monitor and troubleshoot your site’s presence in Google Search results. Here’s how to get started:\nRegister Your Domain:\nAdd your website to GSC and verify ownership. Identify Issues:\nGSC will highlight any indexing or crawling issues. Fix and Optimize:\nAddress the issues to ensure your site is properly indexed. Final Thoughts Ensuring your web app ranks on Google isn’t just about great content—it’s about technical SEO too. By setting the right canonical URLs, dynamically generating sitemaps, and leveraging tools like Google Search Console, you can significantly improve your site’s visibility.\n","permalink":"http://localhost:1313/posts/seo-google-search-console/","summary":"\u003ch1 id=\"custom-solutions-to-enhance-search-enginer-optimization-and-an-overview-of-google-seach-console\"\u003eCustom solutions to enhance Search Enginer Optimization and an Overview of Google Seach Console\u003c/h1\u003e\n\u003cp\u003eImagine this: Your web app is ready to rock the world. You’ve nailed the development, deployment is flawless, and your marketing team is prepped to take the spotlight. But there’s a catch—when users Google your app’s name, your website doesn’t show up. How can users trust your app is legit? And if they want to test your product, how will they find it? You can’t just hand out your server’s IP address to bypass DNS, right?\u003c/p\u003e","title":"How to Ensure Your Websites Ranks on Google"},{"content":"A Developer’s Best Friend for Linux Systems If you’ve ever deployed something on the web, you’re likely familiar with servers running Linux distributions. Unlike your local machine, these servers often lack a graphical user interface (GUI). Instead, you interact with them through a terminal window. This is where SHELL comes into play.\nSHELL is the language of Linux that allows you to interact with the system, execute commands, and automate repetitive tasks. Whether you’re managing files, deploying applications, or setting up servers, SHELL scripting can save you time and effort.\nWhat is SHELL? SHELL is a command-line interpreter that acts as a bridge between you and the operating system. It allows you to execute commands, manage files, and automate tasks. Think of it as the \u0026ldquo;language\u0026rdquo; you use to communicate with Linux-based systems.\nWhy is SHELL Important? No GUI? No Problem: Servers often run without a GUI, so SHELL is your primary way to interact with them. Automation: Repetitive tasks can be automated using SHELL scripts, reducing the chance of human error. Efficiency: A single SHELL script can perform complex operations in seconds, saving you hours of manual work. My First Encounter with SHELL’s Power I truly understood the power of SHELL scripting when I was tasked with setting up a server for our European user base. Here’s what the process looked like:\nCreate Folders: Set up directories for the backend server code and future changes. Install AWS CLI: Interact with Amazon ECR (Elastic Container Registry) to fetch Docker images. Configure AWS CLI: Authenticate and configure the AWS CLI. Install Docker: Ensure Docker is installed and running. Fetch and Run Docker Images: Pull the required Docker image and run it with the correct port mappings. Manually performing these steps for every server would have been tedious and error-prone. But with SHELL scripting, I automated the entire process into a single command. Here’s how:\n#!/bin/bash # Step 1: Create folders for the backend server code base and future changes CODE_BASE_DIR=\u0026#34;/opt/backend_server\u0026#34; CHANGES_DIR=\u0026#34;/opt/backend_changes\u0026#34; mkdir -p \u0026#34;$CODE_BASE_DIR\u0026#34; \u0026#34;$CHANGES_DIR\u0026#34; echo \u0026#34;Created folders: $CODE_BASE_DIR and $CHANGES_DIR\u0026#34; # Step 2: Install AWS CLI if not already installed if ! command -v aws \u0026amp;\u0026gt; /dev/null; then echo \u0026#34;AWS CLI not found. Installing...\u0026#34; curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install rm -rf awscliv2.zip aws/ echo \u0026#34;AWS CLI installed.\u0026#34; else echo \u0026#34;AWS CLI is already installed.\u0026#34; fi # Step 3: Configure AWS CLI (Requires user input) echo \u0026#34;Configuring AWS CLI...\u0026#34; aws configure # Step 4: Authenticate Docker with Amazon ECR if ! command -v docker \u0026amp;\u0026gt; /dev/null; then echo \u0026#34;Docker not found. Installing...\u0026#34; sudo apt-get update sudo apt-get install -y docker.io sudo systemctl start docker sudo systemctl enable docker echo \u0026#34;Docker installed and running.\u0026#34; else echo \u0026#34;Docker is already installed.\u0026#34; fi AWS_REGION=\u0026#34;us-east-1\u0026#34; # Change this to your region ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) ECR_URL=\u0026#34;$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\u0026#34; aws ecr get-login-password --region $AWS_REGION | sudo docker login --username AWS --password-stdin $ECR_URL # Step 5: Fetch the image and run it IMAGE_NAME=\u0026#34;backend-server\u0026#34; # Replace with your image name TAG=\u0026#34;latest\u0026#34; # Replace with your desired tag FULL_IMAGE_NAME=\u0026#34;$ECR_URL/$IMAGE_NAME:$TAG\u0026#34; echo \u0026#34;Pulling image: $FULL_IMAGE_NAME\u0026#34; sudo docker pull \u0026#34;$FULL_IMAGE_NAME\u0026#34; # Step 6: Run the Docker container and expose ports PORTS=(8080:8080 3000:3000 5432:5432) # Add your specific port mappings here PORT_ARGS=\u0026#34;\u0026#34; for PORT in \u0026#34;${PORTS[@]}\u0026#34;; do PORT_ARGS+=\u0026#34;-p $PORT \u0026#34; done echo \u0026#34;Running Docker container with port mappings: ${PORTS[*]}\u0026#34; sudo docker run -d $PORT_ARGS --name backend_server \u0026#34;$FULL_IMAGE_NAME\u0026#34; echo \u0026#34;Setup completed. Backend server is up and running.\u0026#34; This script automates the entire server setup process, ensuring consistency and eliminating manual errors.\nAnother Real-World Use Case: Automating Netdata Installation In the world of microservices, monitoring tools like Netdata are essential. Netdata provides real-time insights into API throughput, CPU usage, disk usage, and more. It can even send alerts (e.g., Slack notifications) when thresholds are exceeded.\nThe Challenge We had 6-7 interdependent services, and I was tasked with installing Netdata on each one. Each service needed to transmit its data to a central \u0026ldquo;parent\u0026rdquo; Netdata node. Manually configuring each service would have been time-consuming and error-prone.\nThe Solution: SHELL Scripting I created a SHELL script to automate the installation and configuration of Netdata across all services. Here’s what the script did:\nInstall Netdata: Automatically install Netdata on each service. Configure Parent Node: Set the parent Netdata node for data transmission. Ensure Uniformity: Standardize the setup process for future services. This not only saved time but also ensured consistency across all services.\nWhy Every Developer Should Learn SHELL SHELL scripting is like Git or SQL—it’s a foundational skill every developer should have. Whether you’re deploying applications, managing servers, or automating tasks, SHELL can make your life easier.\nKey Benefits of SHELL Scripting Automation: Automate repetitive tasks to save time and reduce errors. Efficiency: Perform complex operations with a single command. Flexibility: SHELL scripts can be used for a wide range of tasks, from file management to server setup. SHELL scripting is a powerful tool that every developer should have in their toolkit. Whether you’re setting up servers, automating deployments, or managing microservices, SHELL can help you work smarter, not harder.\n","permalink":"http://localhost:1313/posts/shell-scripting/","summary":"\u003ch1 id=\"a-developers-best-friend-for-linux-systems\"\u003eA Developer’s Best Friend for Linux Systems\u003c/h1\u003e\n\u003cp\u003eIf you’ve ever deployed something on the web, you’re likely familiar with servers running Linux distributions. Unlike your local machine, these servers often lack a graphical user interface (GUI). Instead, you interact with them through a terminal window. This is where \u003cstrong\u003eSHELL\u003c/strong\u003e comes into play.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Terminal using Shell script\" loading=\"lazy\" src=\"/images/terminal-shell.png\"\u003e\u003c/p\u003e\n\u003cp\u003eSHELL is the language of Linux that allows you to interact with the system, execute commands, and automate repetitive tasks. Whether you’re managing files, deploying applications, or setting up servers, SHELL scripting can save you time and effort.\u003c/p\u003e","title":"Why and What is SHELL?"},{"content":"The F Pixel by Meta Yes, it\u0026rsquo;s a Facebook version of Mixpanel or a data analytics tool used to track user behavior across app, web, iOS, or any device with Chrome. The purpose is to collect user data—of course, with consent (whether to be tracked or not 🧑‍💻)—to help business owners run personalized ads and campaigns via Facebook.\nIt’s as simple to use as embedding an image. This is because, theoretically, for all practical purposes, it’s just a 1px x 1px pixel embedded in your HTML DOM, along with some scripts that send data to Facebook\u0026rsquo;s servers.\nThis is what a FB pixel looks like technically -\n\u0026lt;script\u0026gt; !function(f,b,e,v,n,t,s) {if(f.fbq)return;n=f.fbq=function(){n.callMethod? n.callMethod.apply(n,arguments):n.queue.push(arguments)}; if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0'; n.queue=[];t=b.createElement(e);t.async=!0; t.src=v;s=b.getElementsByTagName(e)[0]; s.parentNode.insertBefore(t,s)}(window, document,'script', '[https://connect.facebook.net/en_US/fbevents.js](https://connect.facebook.net/en_US/fbevents.js)'); fbq('init', [This is Pixel id]); fbq('track', 'PageView'); \u0026lt;/script\u0026gt; \u0026lt;noscript\u0026gt;\u0026lt;img height=\u0026quot;1\u0026quot; width=\u0026quot;1\u0026quot; style=\u0026quot;display:none\u0026quot; src=\u0026quot;[https://www.facebook.com/tr?id=[This is Pixel id]\u0026amp;ev=PageView\u0026amp;noscript=1](https://www.facebook.com/tr?id=[this is pixel id]\u0026amp;ev=PageView\u0026amp;noscript=1)\u0026quot; /\u0026gt;\u0026lt;/noscript\u0026gt; \u0026lt;!-- End Meta Pixel Code --\u0026gt; To get started, you need to register a pixel for yourself on Facebook\u0026rsquo;s Events Manager website. Once added, you can view the source code and see graphs of user activities based on the custom event triggers you’ve set, which we\u0026rsquo;ll discuss later.\nAfter registering your web/app/iOS application, you\u0026rsquo;ll receive a unique Meta Pixel ID. This ID helps connect that pixel to your Events Manager database.\nHowever, like many things in life, nothing lasts forever. The U.S. Department of State raised concerns with Facebook regarding user privacy, and the folks at Apple are very conscious about this—likely due to marketing. Since 2021, a lot has changed regarding tracking, especially on newer devices.\nTo handle this, there’s something called the Conversion API, which works similarly to Mixpanel for devices affected by these changes.\nThings to keep in mind:\nRegister your Android/iOS app with Meta Developers and use their SDK to track events. For the web, there\u0026rsquo;s a browser extension that lets you monitor the status of the Meta Pixel. F Pixel in NextJs There is an example repository by the official Vercel team that I highly recommend checking out if you want to use Facebook Pixel with modern web frameworks: Repo Link\nAfter setting up the boilerplate HTML scripts in your Next.js app, create a file, let’s say named Fpixel.ts, with the following contents:\nexport const FB_PIXEL_ID = process.env.NEXT_PUBLIC_FACEBOOK_PIXEL_ID; export const pageview = () =\u0026gt; { // this is to init tracking window.fbq(\u0026quot;track\u0026quot;, \u0026quot;PageView\u0026quot;); }; export const event = (name: any, options = {}) =\u0026gt; { //. This is to trigger an event to facebook servers window.fbq(\u0026quot;track\u0026quot;, name, options); }; After setting up these two functions, you\u0026rsquo;re mostly good to go. Now, you just need to call the pageview function when a page is mounted. As you guessed, the best place to do this is within the useEffect hook with an empty dependency array in app.tsx.\nIf you want to trigger an event on a checkout button in your app, you can do so as follows:\nuseEffect(() =\u0026gt; { fbq.pageview(); const handleRouteChange = () =\u0026gt; { fbq.pageview(); }; router.events.on(\u0026quot;routeChangeComplete\u0026quot;, handleRouteChange); // Trigger an event to pixel on route change return () =\u0026gt; { router.events.off(\u0026quot;routeChangeComplete\u0026quot;, handleRouteChange); }; }, [router.events]); Similarly you can trigger an event let say on payment button to know how many users clicked on pay button coming from your adds run using facebook\nimport React, { useEffect } from 'react'; // Facebook Pixel configuration export const FB_PIXEL_ID = process.env.NEXT_PUBLIC_FACEBOOK_PIXEL_ID; const pageview = () =\u0026gt; { window.fbq('track', 'PageView'); }; const event = (name: string, options = {}) =\u0026gt; { window.fbq('track', name, options); }; // Main Component const FacebookPixelExample: React.FC = () =\u0026gt; { useEffect(() =\u0026gt; { // Track PageView on component load pageview(); }, []); // Custom event handler for button click const handleButtonClick = () =\u0026gt; { event('ButtonClick', { customParam: 'customValue' }); console.log('Facebook Pixel event tracked: ButtonClick'); }; return ( \u0026lt;div\u0026gt; \u0026lt;h1\u0026gt;Facebook Pixel Integration\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Click the button to trigger a custom Facebook Pixel event.\u0026lt;/p\u0026gt; \u0026lt;button onClick={handleButtonClick}\u0026gt; Payment \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt;); }; export default FacebookPixelExample; So there you have it—Facebook or Meta Pixel implementation in Next.js or any other modern framework. Additionally, during my testing with Meta Pixel, I noticed that it does not work with any type of ad blocker installed in the user\u0026rsquo;s browser.\n","permalink":"http://localhost:1313/posts/meta-pixel/","summary":"\u003ch2 id=\"the-f-pixel-by-meta\"\u003eThe F Pixel by Meta\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"Facebook Pixel by Meta\" loading=\"lazy\" src=\"/images/meta_pixel.webp\"\u003e\u003c/p\u003e\n\u003cp\u003eYes, it\u0026rsquo;s a Facebook version of Mixpanel or a data analytics tool used to track user behavior across app, web, iOS, or any device with Chrome. The purpose is to collect user data—of course, with consent (whether to be tracked or not 🧑‍💻)—to help business owners run personalized ads and campaigns via Facebook.\u003c/p\u003e\n\u003cp\u003eIt’s as simple to use as embedding an image. This is because, theoretically, for all practical purposes, it’s just a 1px x 1px pixel embedded in your HTML DOM, along with some scripts that send data to Facebook\u0026rsquo;s servers.\u003c/p\u003e","title":"The F Pixel by Meta"},{"content":"Navigator API - More than meets the Eye For those who don’t know, the Navigator API is a native API available in all browsers that provides user-centric metrics for your website. It offers insights into how your high-value site is performing. Some may use it to customize the interface for different platforms, such as displaying premium items to iOS users, but its main purpose is to understand how your code behaves across different user environments and internet speeds, including ping. This helps developers diagnose and recreate issues to solve them more effectively. The Navigator API also provides CPU heap statistics, which reveal how CSS effects impact users\u0026rsquo; CPU threads.\nBy now, you might have a better understanding of what the Navigator API is and why it’s valuable. Let’s see how to incorporate it into your million dollar web app.\nFor practical use—specifically the task I was assigned—we use Sentry for our app, which does an excellent job tracking errors. However, knowing about an error is only part of the story; additional information is crucial for effective troubleshooting. Key statistics like network speed at the time of the error, heap size, or RAM usage on the device are needed.\nLet’s address the key aspects one by one:\nEnvironment/Model - navigator.userAgent This is the most crucial information. It reveals details about the operating system on which the web page is being viewed, as well as key information about the manufacturer, platform, and web view version.\nconst userAgent = navigator.userAgent; console.log(userAgent); //console output // { // \u0026#34;isVirtual\u0026#34;: false, // \u0026#34;manufacturer\u0026#34;: \u0026#34;Google Inc.\u0026#34;, // \u0026#34;model\u0026#34;: \u0026#34;Macintosh\u0026#34;, // \u0026#34;operatingSystem\u0026#34;: \u0026#34;mac\u0026#34;, // \u0026#34;osVersion\u0026#34;: \u0026#34;10.15.7\u0026#34;, // \u0026#34;platform\u0026#34;: \u0026#34;web\u0026#34;, // \u0026#34;webViewVersion\u0026#34;: \u0026#34;127.0.0.0\u0026#34; // }, CPU heap Size - `` The memory heap is the portion of memory allocated to the JavaScript runtime to store all objects, strings, and data structures. In simple terms, the more memory allocated, the more processing power your processor will need to handle the calculations and data management.\nconst userCPUHeap = performance.memory; console.log(userCPUHeap); //console output // { // \u0026#34;totalJSHeapSize\u0026#34;: 124342323 Bytes, // \u0026#34;totalJSHeapSize\u0026#34;: 66605138 Bytes, // \u0026#34;usedJSHeapSize\u0026#34;: 51178854 Bytes, // }, RAM This gives the at-least physical hardware RAM memory in GB of the user machine. Unlike JS heap size it value is is in range [2,3,4,8]. For devices greater than 8 this method with return 8 .\nconst userPhysicalRAM = navigator.deviceMemory; console.log(userPhysicalRAM); //console output // { // RAM: 8 // } Network Stats If you want to measure the user\u0026rsquo;s network speed while loading your website, this is a key statistic for tracking and monitoring your web vitals based on the user\u0026rsquo;s average network speed. While one method is to download a test file of known size and measure the time taken for the operation, this approach can increase the app size and waste user resources.\nconst networkInfo = \u0026#34;connection\u0026#34; in navigator ? navigator.connection || navigator.mozConnection || navigator.webkitConnection : null; const userNetworkInfo = { effectiveType: networkInfo ? networkInfo.effectiveType : \u0026#34;Not available\u0026#34;, downlink: networkInfo ? networkInfo.downlink : \u0026#34;Not available\u0026#34;, rtt: networkInfo ? networkInfo.rtt : \u0026#34;Not available\u0026#34;, }; console.log(userNetworkInfo); // Example console output // { // effectiveType: \u0026#39;4g\u0026#39;, Network Type of user Wifi or Band // downlink: 10, This is the avg Speed calculated by Chrome // rtt: 50 // } The downlink field provides the approximate bandwidth of the client\u0026rsquo;s connection to the server, measured in Mbps, while the RTT field estimates the round-trip time in milliseconds (ms) for the network.\nFor practical purposes, these metrics are essential for tracking user performance. Remember to append this data to your Mixpanel events for comprehensive data analytics which we did in our systems.\nAlert 🍎 (For Apple enthusiasts): Due to privacy constraints, some API endpoints are not supported by Safari. This means you won’t be able to obtain certain information, such as network speed, RAM, or CPU heap size, as of now. Good luck with that! 😊\n","permalink":"http://localhost:1313/posts/navigator-api/","summary":"\u003ch1 id=\"navigator-api---more-than-meets-the-eye\"\u003eNavigator API - More than meets the Eye\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"Navigator API present inside Chrome Dev\" loading=\"lazy\" src=\"/images/chrome_dev.png\"\u003e\u003c/p\u003e\n\u003cp\u003eFor those who don’t know, the Navigator API is a native API available in all browsers that provides user-centric metrics for your website. It offers insights into how your high-value site is performing. Some may use it to customize the interface for different platforms, such as displaying premium items to iOS users, but its main purpose is to understand how your code behaves across different user environments and internet speeds, including ping. This helps developers diagnose and recreate issues to solve them more effectively. The Navigator API also provides CPU heap statistics, which reveal how CSS effects impact users\u0026rsquo; CPU threads.\u003c/p\u003e","title":"Know your user like never before using Navigator API"},{"content":"\nShortly after joining a Bangalore startup after college, due to my previous experience in working with auth systems, I was given the task to change the current auth system from admin-secret based to a token-based auth system. with our prod server having an active user base of 5000 plus users daily.\nAdding more spice to the curry, since the app is used by children, we don’t want them to be redirected to a login screen after the new auth system is implemented. The transition for auth in both our frontend and backend systems should happen frictionlessly without users even noticing.\nSo, let’s begin on how we can achieve that.\nI chose SuperTokens for this task as it takes care of a lot of things that, as a developer, I don’t have to make from scratch. SuperTokens has methods they call recipes, and I used the Passwordless phone method, which will require users to provide their phone number and, after OTP verification, will initiate a session.\nThis blog is not about how you can use SuperTokens; I am sure there are plenty of resources out there that teach you that. I am going to tell you a clever way to migrate your current auth system to a new one without users even noticing.\nShortcut Way - Migrate your user database to a new auth system. Yes, my friend, one might think this is the best way to do it. Many auth systems have an export user table feature, and you can just transfer it. Congratulations, you have shifted the user to a new auth system. But what if, during transfers, someone signs up on the platform? You are going to lose that. Also, for an organization with such a large user base and many new users coming every day, we don’t want the experience to be spoiled for anyone.\nClever Way - Consider two versions of your frontend client, A and B. Version A has the current auth system that you want to migrate, and version B has the new auth system set up by you. For simplicity\u0026rsquo;s sake, let’s assume you have created a new auth system named C that handles sessions, tokens, post-sign-in, sign-up callbacks, etc.\nStep 1: Base Case - Test out frontend B (the one with the new auth system) with backend C. Ideally, this will be the future of your application. The frontend has the new auth system, and the backend is configured with it. Test to ensure everything works. In my case, I replaced localStorage with a custom hook that sends session-related information to the frontend for any request made to our DB (Hasura).\nStep 2: Make an Endpoint in your backend system C which will take user info, e.g., phone number or any unique identifier of your choice, and respond with an access token that can be stored in cookies of any system that calls it. It is just like a sign-up post route but without verification.\n// generate token endpoint inside nodejs express app export const generatetoken = async (req, res) =\u0026gt; { try { let phoneNumber = req.body.phoneNumber; // unique identifier in our case const input = { tenantId: `public`, // important for supertokens phoneNumber: phoneNumber, }; let signupResponse = await signInUp(input); // sign-up function provided by supertokens if (signupResponse.status != \u0026#34;OK\u0026#34;) { res.json(\u0026#34;message: error in creating tokens\u0026#34;); } const supertokenID = signupResponse.user.id; // user id created inside new auth system let session = await Session.createNewSessionWithoutRequestResponse( \u0026#34;public\u0026#34;, supertokens.convertToRecipeUserId(supertokenID) ); if (!session) { throw new Error(\u0026#34;Session could not be created\u0026#34;); } // Sending userInfo in the response as token res.json({ message: \u0026#34;Token generation done\u0026#34;, sessionToken: session }); } catch (error) { console.error(\u0026#34;Error generating token:\u0026#34;, error); res.status(500).send(error); } }; Step 3: Integrate this Endpoint into the current frontend with the old auth system ‘A’. In my case, I made a script using useEffect that makes a request to this endpoint I created when the app mounts. Note: This endpoint will only work for users who have already signed in to the application. We don’t want to generate tokens for every user using the application; otherwise, what kind of authentication system is it? :-) // Script inside system where migration needs to be done, i.e., \u0026#39;A\u0026#39; export const checkOrSetCookie = async () =\u0026gt; { const user = localStorage.getItem(\u0026#34;personalized_student\u0026#34;); // method to get unique identifier in old auth system if (user) { const { mobile: phoneNumber } = JSON.parse(user); if (phoneNumber) { try { const accessToken = await generateSessionToken(phoneNumber); // making request to above created endpoint if (!accessToken) { throw new Error(\u0026#34;Failed to generate access token\u0026#34;); } const userCookie = getCookie(\u0026#34;personalized_student\u0026#34;); if (userCookie) return; document.cookie = `personalized_student=${user}; domain=.yourDomain.com;`; // storing access token inside cookie document.cookie = `accesstoken=${accessToken}; domain=.yourDomain.com;`; } catch (error) { console.error(\u0026#34;Error setting cookie:\u0026#34;, error); } } } }; Step 4: Run the Above Script for 1 week in your existing system ‘A’ on production. By now, you have generated tokens for at least 99% of users using your app. Note: The tokens generated are stored inside the HTTP-only cookie of the user with an expiration time long enough before executing the next step.\nStep 5: Release Version B of your frontend to production with an extra script to check for the token inside cookies. We call this script a migrate script. Also, you need to create an endpoint in the backend system which will accept tokens and initiate a session for the user making the request. If the token is found inside the cookies (which we did in Step 3), then do nothing. Congratulations, you have successfully migrated your user frictionlessly without them noticing anything. If not, then it might be a new user or the unlucky 1% who will be redirected to the sign-up page with the new auth system.\nuseEffect(() =\u0026gt; { async function initializeSession() { // Ensure access token is set in the cookie function getCookie(name: string) { const value = `; ${document.cookie}`; const parts = value.split(`; ${name}=`); if (parts.length === 2) return parts?.pop()?.split(\u0026#34;;\u0026#34;)?.shift(); return null; } // Attempt to refresh session const accessToken = getCookie(\u0026#34;accesstoken\u0026#34;); if (accessToken) { try { const response = await axios.post( `${process.env.NEXT_PUBLIC_SUPERTOKEN_API_BACKEND_DOMAIN}/supertoken/migrate`, {}, { headers: { Authorization: `Bearer ${accessToken}`, }, } ); if (response.data.status != \u0026#34;OK\u0026#34;) { throw new Error(\u0026#34;No Supertoken session started\u0026#34;); } console.log(\u0026#34;Supertoken session established\u0026#34;, response); } catch (err) { console.error(\u0026#34;Error refreshing session\u0026#34;, err); } } else { window.redirect(\u0026#34;/signup\u0026#34;); // this is optional as you may have your own route protection } } initializeSession(); }, []); ","permalink":"http://localhost:1313/posts/migrating-auth/","summary":"\u003cp\u003e\u003cimg alt=\"Auth System Migration with supertoken\" loading=\"lazy\" src=\"/images/auth-supertokens.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eShortly after joining a Bangalore startup after college, due to my previous experience in working with auth systems, I was given the task to change the current auth system from admin-secret based to a token-based auth system. with our prod server having an active user base of 5000 plus users daily.\u003c/p\u003e\n\u003cp\u003eAdding more spice to the curry, since the app is used by children, we don’t want them to be redirected to a login screen after the new auth system is implemented. The transition for auth in both our frontend and backend systems should happen frictionlessly without users even noticing.\u003c/p\u003e","title":"Seamless Auth System Migration with Supertokens"},{"content":"Introduction Emojis have become a significant part of digital communication, bringing a fun and expressive element to text. In web development, enhancing user experience and engagement is crucial. One creative way to achieve this is by integrating emojis into URL routes. This article explores the concept, benefits, and implementation of emojis in URL routes for your next web app project.\nUnderstanding URL Routes URL routes are the paths defined in your web application that map to different content or functionality. Traditional URL structures, like /about or /contact, are straightforward but can be bland. Adding emojis can make these routes more engaging and memorable.\nBenefits of Using Emojis in URLs Enhanced Engagement: Emojis can make URLs more eye-catching and memorable. Improved Aesthetics: Modern and visually appealing URL structures. Potential SEO Benefits: Emojis can enhance click-through rates if used thoughtfully. Technical Implementation Setting Up a Basic Web App For this example, we\u0026rsquo;ll use Next.js, a popular React framework.\nFirst, create a new Next.js project:\nnpx create-next-app@latest emoji-url-app cd emoji-url-app Modifying Routes to Include Emojis Next.js uses file-based routing, so we can create a new page with an emoji in the filename.\nCreate a new file in the pages directory, named 🍕.js: // pages/🍕.js export default function PizzaPage() { return ( \u0026lt;div\u0026gt; \u0026lt;h1\u0026gt;Welcome to the Pizza Page! 🍕\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;This URL contains an emoji!\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ); }` Start the development server: npm run dev Navigate to http://localhost:3000/🍕 to see your emoji route in action.\nCode Snippets and Examples Here\u0026rsquo;s another example using Express.js:\nconst express = require(\u0026#39;express\u0026#39;); const app = express(); const port = 3000; // Route with emoji app.get(\u0026#39;/🍕\u0026#39;, (req, res) =\u0026gt; { res.send(\u0026#39;Welcome to the Pizza Page! 🍕\u0026#39;); }); app.listen(port, () =\u0026gt; { console.log(`Example app listening at http://localhost:${port}/🍕`); }); Browser and Server Considerations Compatibility Issues and Solutions Not all browsers and servers handle emojis in URLs the same way. Use URL encoding to ensure compatibility:\nconst encodedURL = encodeURI(\u0026#39;/🍕\u0026#39;); app.get(encodedURL, (req, res) =\u0026gt; { res.send(\u0026#39;Welcome to the Pizza Page! 🍕\u0026#39;); }); Ensuring Server-Side Support Ensure your server and framework support UTF-8 encoding, which is necessary for emojis. Most modern frameworks and servers do this by default.\nTesting and Debugging Test your emoji URLs across different browsers and devices to ensure consistent behavior. Use online tools like Emoji URL Encoder for manual testing.\nConclusion Integrating emojis into your URL routes can significantly enhance user experience and engagement. While there are technical considerations and best practices to follow, the benefits of memorable and visually appealing URLs are worth exploring. Start experimenting with emoji routes in your web app today and stay ahead of web development trends.\n","permalink":"http://localhost:1313/posts/emojiintheurl/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eEmojis have become a significant part of digital communication, bringing a fun and expressive element to text. In web development, enhancing user experience and engagement is crucial. One creative way to achieve this is by integrating emojis into URL routes. This article explores the concept, benefits, and implementation of emojis in URL routes for your next web app project.\u003c/p\u003e\n\u003ch2 id=\"understanding-url-routes\"\u003eUnderstanding URL Routes\u003c/h2\u003e\n\u003cp\u003eURL routes are the paths defined in your web application that map to different content or functionality. Traditional URL structures, like \u003ccode\u003e/about\u003c/code\u003e or \u003ccode\u003e/contact\u003c/code\u003e, are straightforward but can be bland. Adding emojis can make these routes more engaging and memorable.\u003c/p\u003e","title":"Enhancing User Experience: Using Emojis in Web App URLs"},{"content":"Hey readers! I\u0026rsquo;m thrilled to start documenting my learning journey as a web developer, and this blog marks the beginning of that adventure. From now on, I will be sharing my experiences, challenges, and progress as a full stack developer.\nWhy Hugo? You might wonder why I chose Hugo over a more complex web framework. The answer is simple: for a static application like this, Hugo is not only sufficient but also highly efficient. Using more elaborate frameworks for such purposes often feels like overkill. Hugo offers simplicity and speed without compromising performance. In fact, a quick check on Lighthouse shows that all the web vitals are above 97%. Achieving that with more sophisticated frameworks can be quite challenging.\nFeedback and Criticism Welcome I am open to feedback and all the constructive criticism that comes with it. This blog is a platform for me to learn and grow, and your input will be invaluable.\nSummary I’m not a professional writer and don’t aspire to be one. My goal here is to document my learning process in a format that’s more accessible and organized than a collection of code files on GitHub. As a programmer, I often need to revisit past projects, and having a blog will make that much easier. Trust me, it\u0026rsquo;s always those boilerplate syntax details that make me scratch my head.\n","permalink":"http://localhost:1313/posts/helloworld/","summary":"\u003ch2 id=\"hey-readers\"\u003eHey readers!\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m thrilled to start documenting my learning journey as a web developer, and this blog marks the beginning of that adventure. From now on, I will be sharing my experiences, challenges, and progress as a full stack developer.\u003c/p\u003e\n\u003ch2 id=\"why-hugo\"\u003eWhy Hugo?\u003c/h2\u003e\n\u003cp\u003eYou might wonder why I chose Hugo over a more complex web framework. The answer is simple: for a static application like this, Hugo is not only sufficient but also highly efficient. Using more elaborate frameworks for such purposes often feels like overkill. Hugo offers simplicity and speed without compromising performance. In fact, a quick check on Lighthouse shows that all the web vitals are above 97%. Achieving that with more sophisticated frameworks can be quite challenging.\u003c/p\u003e","title":"helloWorld"}]